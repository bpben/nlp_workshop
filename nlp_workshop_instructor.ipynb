{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In your own words: Computing customer similarity using website text data\n",
    "### Workshop developed for DSS Austin '19\n",
    "### By: Ben Batorsky [github](https://github.com/bpben)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Stemmer\n",
    "stemmer = Stemmer.Stemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion\n",
    "The data for the workshop comes from a random set of business website text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = pd.read_pickle('/Users/benjaminbatorsky/Documents/segmentation/data/website_parsed.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "from th_data_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = global_data_loader.read_sql('select * from views.v_get_accounts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_a = w.merge(a[['account_id', 'type']], on='account_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other is going to be missing\n",
    "w_a.loc[w_a.type=='Other*', 'type'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_a_sample = w_a.sample(1000)[['content', 'type']]\n",
    "w_a_sample.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_a_sample.to_pickle(DATA_PATH+'website_text.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_pickle(DATA_PATH+'website_text.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_content = full_data['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Bill': 1, 'paid': 1, 'with': 1, 'a': 1, 'dollar': 1, 'bill': 1})\n",
      "Counter({'bill': 2, 'paid': 1, 'with': 1, 'a': 1, 'dollar': 1})\n"
     ]
    }
   ],
   "source": [
    "# capitalization\n",
    "text = 'Bill paid with a dollar bill'\n",
    "print(Counter(text.split()))\n",
    "print(Counter(text.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy birthday!\n",
      "Happy birthday\n"
     ]
    }
   ],
   "source": [
    "# punctuation\n",
    "#nlp('!').similarity(nlp('.'))\n",
    "text = 'Happy birthday!'\n",
    "strip_punct = '[^A-Za-z0-9 ]'\n",
    "print(text)\n",
    "print(re.sub(strip_punct, '', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call 867-5309\n",
      "Call -\n"
     ]
    }
   ],
   "source": [
    "# numbers\n",
    "text = 'Call 867-5309'\n",
    "strip_num = '[0-9]'\n",
    "print(text)\n",
    "print(re.sub(strip_num, '', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out this conference: https://datascience.salon/austin/\n",
      "Check out this conference: \n"
     ]
    }
   ],
   "source": [
    "# urls\n",
    "SHORT_URL_REGEX = re.compile(\n",
    "    r\"(?:^|(?<![\\w/.]))\"\n",
    "    # optional scheme\n",
    "    r\"(?:(?:https?://)?)\"\n",
    "    # domain\n",
    "    r\"(?:\\w-?)*?\\w+(?:\\.[a-z]{2,12}){1,3}\"\n",
    "    r\"/+\"\n",
    "    # hash\n",
    "    r\"[^\\s.,?!'\\\"|+]{2,12}\"\n",
    "    r\"(?:$|(?![\\w?!+&/]))\",\n",
    "    flags=re.IGNORECASE)\n",
    "text = 'Check out this conference: https://datascience.salon/austin/'\n",
    "print(text)\n",
    "print(SHORT_URL_REGEX.sub('', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Write your preprocessing script\n",
    "Combine some of the regex expressions (or write your own!) to process the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # url\n",
    "    text = SHORT_URL_REGEX.sub('', text)\n",
    "    # numbers\n",
    "    text = re.sub(strip_num, '', text)\n",
    "    # punctuation\n",
    "    text = re.sub(strip_punct, '', text)\n",
    "    # capitalization\n",
    "    text = text.lower()\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_processed = w_content.apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From text to vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 to 1-grams:  ['have', 'lovely', 'bunch', 'of', 'coconuts']\n",
      "1 to 2-grams:  ['have', 'lovely', 'bunch', 'of', 'coconuts', 'have lovely', 'lovely bunch', 'bunch of', 'of coconuts']\n",
      "1 to 3-grams:  ['have', 'lovely', 'bunch', 'of', 'coconuts', 'have lovely', 'lovely bunch', 'bunch of', 'of coconuts', 'have lovely bunch', 'lovely bunch of', 'bunch of coconuts']\n"
     ]
    }
   ],
   "source": [
    "# n-grams\n",
    "text = ['I have a lovely bunch of coconuts']\n",
    "for n in range(1, 4):\n",
    "    vec = CountVectorizer(ngram_range=(1, n))\n",
    "    print('1 to {}-grams: '.format(n), list(vec.fit(text).vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default (no minimum):  ['have', 'lovely', 'bunch', 'of', 'coconuts', 'pears']\n",
      "Appear in >10% of docs:  ['have', 'lovely', 'bunch', 'of', 'coconuts', 'pears']\n",
      "Appear in <90% of docs:  ['pears']\n"
     ]
    }
   ],
   "source": [
    "# choosing cutoffs\n",
    "texts = ['I have a lovely bunch of coconuts']\n",
    "texts = texts*9\n",
    "texts.append('I have a lovely bunch of pears')\n",
    "vec = CountVectorizer()\n",
    "print('Default (no minimum): ', list(vec.fit(texts).vocabulary_.keys()))\n",
    "vec = CountVectorizer(min_df=.1)\n",
    "print('Appear in >10% of docs: ', list(vec.fit(texts).vocabulary_.keys()))\n",
    "vec = CountVectorizer(max_df=.1)\n",
    "print('Appear in <90% of docs: ', list(vec.fit(texts).vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stem of ponies: poni\n",
      "Lemma of ponies: pony\n",
      "\n",
      "Stem of operation: oper\n",
      "Lemma of operation: operation\n",
      "\n",
      "Stem of are: are\n",
      "Lemma of are: be\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# stemming and lemmatizing\n",
    "words = ['ponies', 'operation', 'are']\n",
    "for w in words:\n",
    "    print('Stem of {}: {}'.format(w, stemmer.stemWord(w)))\n",
    "    print('Lemma of {}: {}'.format(w, nlp(w)[0].lemma_))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cowboys ORG\n"
     ]
    }
   ],
   "source": [
    "# entities with spaCy\n",
    "text = \"I'm a Cowboys fan, but I'm not a cowboy\"\n",
    "ents = nlp(text).ents\n",
    "for e in ents:\n",
    "    print(e, e.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['have_a', 'lovely_bunch', 'of_coconuts'],\n",
       " ['have_a', 'lovely_bunch', 'of_coconuts'],\n",
       " ['have_a', 'lovely_bunch', 'of_coconuts'],\n",
       " ['have_a', 'lovely_bunch', 'of_coconuts'],\n",
       " ['have_a', 'lovely_bunch', 'of_pears'],\n",
       " ['have_a', 'lovely_bunch', 'of_pears']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entities with gensim - co-location\n",
    "# This example is a bit odd: Likely not dealing with a bunch of duplicates\n",
    "# Worth noting that, all likelihood being equal, gensim picks the first in a series of bigrams\n",
    "texts = ['have a lovely bunch of coconuts']\n",
    "texts = texts*4\n",
    "texts.append('have a lovely bunch of pears')\n",
    "texts.append('have a lovely bunch of pears')\n",
    "split_texts = [x.split() for x in texts]\n",
    "phrases = Phrases(split_texts, min_count=1, threshold=1)\n",
    "list(phrases[split_texts])\n",
    "#[x for x in phrases.export_phrases([split_texts[0]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'like', 'ice_cream'],\n",
       " ['I', 'will', 'buy', 'you', 'ice_cream'],\n",
       " ['Go', 'to', 'the', 'ice', 'rink'],\n",
       " ['I', 'will', 'buy', 'you', 'ice_cream']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = ['I like ice cream', 'I will buy you ice cream', 'Go to the ice rink', 'I will buy you ice cream',]\n",
    "split_texts = [x.split() for x in texts]\n",
    "phrases = Phrases(split_texts, min_count=2, threshold=1)\n",
    "bigram_sentences = phrases[split_texts]\n",
    "[x for x in bigram_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'ice cream', 1.8333333333333333)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores are based on https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.original_scorer\n",
    "# higher = more likely to be a bigram\n",
    "[x for x in phrases.export_phrases([split_texts[-1]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With stopwords:  ['I', 'have', 'a', 'lovely', 'bunch', 'of', 'coconuts']\n",
      "Without stopwords:  ['lovely', 'bunch', 'coconuts']\n"
     ]
    }
   ],
   "source": [
    "# stopwords\n",
    "#from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "#print(ENGLISH_STOP_WORDS)\n",
    "text = 'I have a lovely bunch of coconuts'\n",
    "vec = CountVectorizer(stop_words='english')\n",
    "print('With stopwords: ', text.split())\n",
    "print('Without stopwords: ', list(vec.fit([text]).vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vectors \n",
      "    are  but  cowboys  cowboysorg  fans  not  we\n",
      "0    2    1        2           0     1    1   2\n",
      "1    2    1        1           1     1    1   2\n",
      "binary vectors \n",
      "    are  but  cowboys  cowboysorg  fans  not  we\n",
      "0    1    1        1           0     1    1   1\n",
      "1    1    1        1           1     1    1   1\n"
     ]
    }
   ],
   "source": [
    "# creating count vectors\n",
    "texts = []\n",
    "text = \"We are Cowboys fans, but we are not cowboys\"\n",
    "text_tagged = \"We are CowboysORG fans, but we are not cowboys\"\n",
    "# phrase without tags, lowercase\n",
    "texts.append(text.lower())\n",
    "# phrase with tags, lowercase\n",
    "texts.append(text_tagged.lower())\n",
    "# utility to display vectorizer\n",
    "def display_vec(vec, data):\n",
    "    df = pd.DataFrame(data.toarray(),\n",
    "                     columns=vec.get_feature_names())\n",
    "    return(df)\n",
    "# count vector\n",
    "vec = CountVectorizer()\n",
    "data = vec.fit_transform(texts)\n",
    "print('count vectors \\n', display_vec(vec, data))\n",
    "# binary count vector\n",
    "b_vec = CountVectorizer(binary=True)\n",
    "data = b_vec.fit_transform(texts)\n",
    "print('binary vectors \\n', display_vec(b_vec, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Remove the stopwords from the above texts\n",
    "Use what we explored above to remove the stopwords from the count vectors of the following texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vectors without stopwords \n",
      "    cowboys  cowboysorg  fans\n",
      "0        2           0     1\n",
      "1        1           1     1\n"
     ]
    }
   ],
   "source": [
    "texts = ['we are cowboys fans, but we are not cowboys',\n",
    " 'we are cowboysorg fans, but we are not cowboys']\n",
    "\n",
    "nostop_vec = CountVectorizer(stop_words='english')\n",
    "data = nostop_vec.fit_transform(texts)\n",
    "print('count vectors without stopwords \\n', display_vec(nostop_vec, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Set limits on the vocabulary to remove potentially irrelevant words\n",
    "With the following set of texts, set a limit to remove unimportant words like \"Patriots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vector with vocab limit \n",
      "    are  cowboys  fans  we\n",
      "0    1        1     1   1\n",
      "1    1        1     0   1\n",
      "2    1        0     1   1\n"
     ]
    }
   ],
   "source": [
    "texts = ['We are Cowboys fans',\n",
    "        'We are cowboys',\n",
    "        'We are Patriots fans']\n",
    "texts_lower = [t.lower() for t in texts]\n",
    "\n",
    "limit_vec = CountVectorizer(min_df=2)\n",
    "data = limit_vec.fit_transform(texts)\n",
    "print('Count vector with vocab limit \\n', display_vec(limit_vec, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF weighting\n",
    "TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['We are Cowboys fans',\n",
    "         'We are Patriots fans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   are  cowboys  fans  patriots  we\n",
      "0    1        1     1         0   1\n",
      "1    1        0     1         1   1\n"
     ]
    }
   ],
   "source": [
    "# calculate term frequency\n",
    "vec = CountVectorizer()\n",
    "count_vectors = vec.fit_transform(texts)\n",
    "count_df = display_vec(vec, count_vectors)\n",
    "print(count_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula for inverse document frequency weight:\n",
    "\n",
    "$$log(\\frac{N}{df(t)}) + 1$$\n",
    "\n",
    "\"smooth\" option ensures no zero-divisions:\n",
    "\n",
    "$$log(\\frac{N+1}{df(t)+1}) + 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do'"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('doing')[0].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are         1.000000\n",
      "cowboys     1.405465\n",
      "fans        1.000000\n",
      "patriots    1.405465\n",
      "we          1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# get document frequency\n",
    "df = np.log(3/(1+count_df.sum()))+1\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        are   cowboys      fans  patriots        we\n",
      "0  0.448321  0.630099  0.448321  0.000000  0.448321\n",
      "1  0.448321  0.000000  0.448321  0.630099  0.448321\n"
     ]
    }
   ],
   "source": [
    "# calculate tfidf\n",
    "tfidf_df = count_df*df\n",
    "# normalize\n",
    "print(tfidf_df.apply(\n",
    "    lambda x: x/np.sqrt(x.dot(x)), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        are   cowboys      fans  patriots        we\n",
      "0  0.448321  0.630099  0.448321  0.000000  0.448321\n",
      "1  0.448321  0.000000  0.448321  0.630099  0.448321\n"
     ]
    }
   ],
   "source": [
    "# now with scikit-learn\n",
    "tfidf = TfidfVectorizer()\n",
    "data = tfidf.fit_transform(texts)\n",
    "print(display_vec(tfidf, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF weighted count vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Turn text to vectors\n",
    "Using what we've gone through above, create your own count vectorizer and TFIDF vectorizer.  Apply these vectorizers to the data, and store the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_params = {'min_df': .005, 'max_df': .3, 'stop_words':'english'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(**vector_params)\n",
    "tfidf_vectorizer = TfidfVectorizer(**vector_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vecs = count_vectorizer.fit_transform(w_content)\n",
    "tfidf_vecs = tfidf_vectorizer.fit_transform(w_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix factorization and topic modelling\n",
    "\n",
    "#### Latent Semantic Indexing\n",
    "In scikit-learn this is implemented as TruncatedSVD, a version of SVD where the top k elements are retained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=2, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TruncatedSVD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics_s = 10\n",
    "mparams = {'n_components':n_topics_s,  'init':'nndsvd'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>11am</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>24</th>\n",
       "      <th>30</th>\n",
       "      <th>40</th>\n",
       "      <th>573</th>\n",
       "      <th>607</th>\n",
       "      <th>614</th>\n",
       "      <th>660</th>\n",
       "      <th>...</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>welcome</th>\n",
       "      <th>wellbeing</th>\n",
       "      <th>wings</th>\n",
       "      <th>winter</th>\n",
       "      <th>world</th>\n",
       "      <th>wraps</th>\n",
       "      <th>wrong</th>\n",
       "      <th>york</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094364</td>\n",
       "      <td>0.03339</td>\n",
       "      <td>0.049004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.148714</td>\n",
       "      <td>0.088588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.213669</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.206162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095301</td>\n",
       "      <td>0.076279</td>\n",
       "      <td>0.148714</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034342</td>\n",
       "      <td>0.030106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.10143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152498</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 335 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       11am        12        13        24        30        40       573  \\\n",
       "0  0.000000  0.000000  0.033591  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.148714  0.088588  0.000000  0.000000  0.000000  0.000000  0.213669   \n",
       "2  0.000000  0.000000  0.000000  0.034342  0.030106  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.058597  0.000000   \n",
       "\n",
       "       607       614       660    ...         want       way   welcome  \\\n",
       "0  0.00000  0.000000  0.000000    ...     0.000000  0.021712  0.000000   \n",
       "1  0.00000  0.206162  0.000000    ...     0.031885  0.000000  0.095301   \n",
       "2  0.10143  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "3  0.00000  0.000000  0.082193    ...     0.000000  0.000000  0.037349   \n",
       "\n",
       "   wellbeing     wings   winter     world     wraps     wrong      york  \n",
       "0   0.000000  0.094364  0.03339  0.049004  0.000000  0.000000  0.000000  \n",
       "1   0.076279  0.148714  0.00000  0.000000  0.068721  0.000000  0.000000  \n",
       "2   0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.040288  \n",
       "3   0.000000  0.000000  0.00000  0.000000  0.000000  0.152498  0.000000  \n",
       "\n",
       "[4 rows x 335 columns]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# couple example website text\n",
    "# these come from the main data: indices = 723, 826, 900, 974\n",
    "idxs = [723, 826, 900, 974]\n",
    "example_texts = w_content.loc[idxs]\n",
    "# LSI requires tfidf-weighted vectors, use from above\n",
    "tfidf_example = tfidf_vectorizer.transform(example_texts)\n",
    "# create display of examples\n",
    "display_example = display_vec(tfidf_vectorizer, tfidf_example)\n",
    "# for clarity, drop vocab that does not occur\n",
    "display_example.loc[:, (display_example.sum(axis=0)>0).values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_components(model, word_features, top_display=5):\n",
    "    # utility for displaying respresentative words per component\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        top_words_idx = topic.argsort()[::-1][:top_display]\n",
    "        top_words = [word_features[i] for i in top_words_idx]\n",
    "        print(\" \".join(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "pizza columbia brick wings tasty\n",
      "Topic 1:\n",
      "brake repair trucks automotive fleet\n",
      "Topic 2:\n",
      "trucks fleet towing heavy duty\n",
      "Topic 3:\n",
      "columbia events sunrise 573 614\n"
     ]
    }
   ],
   "source": [
    "# specify number of components\n",
    "n_components = 4\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "svd_example = svd.fit_transform(tfidf_example)\n",
    "display_components(svd, tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old-Fashioned Brick Oven Pizza in Adrian, MI | Piz [ 0.81355706 -0.1577552   0.09633749 -0.55132323]\n",
      "Pizza and Wings Delivery Columbia MO Kostaki's Piz [ 0.82427545 -0.09087286 -0.04062145  0.55737062]\n",
      "Truck, trailer, and auto repair | Sonny’s Service  [0.10128071 0.75363889 0.64855891 0.03379328]\n",
      "Auto Repair, Kirksville, MO Kirksville Brake & Muf [ 0.17148069  0.74013027 -0.64484981 -0.08348823]\n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(example_texts):\n",
    "    print(t[:50], svd_example[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Create LSI vectors\n",
    "Using the TFIDF vectors from above, create LSI vectors for the website text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# likely better to use more than 5 components\n",
    "n_components = 10\n",
    "lsi = TruncatedSVD(n_components=n_components)\n",
    "lsi_vecs = lsi.fit_transform(tfidf_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x4891 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 106980 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "design care 00 insurance repair\n",
      "Topic 1:\n",
      "insurance law financial health estate\n",
      "Topic 2:\n",
      "insurance law estate painting legal\n",
      "Topic 3:\n",
      "insurance repair auto car care\n",
      "Topic 4:\n",
      "insurance design interior marketing portfolio\n",
      "Topic 5:\n",
      "jpg squarespace static1 static https\n",
      "Topic 6:\n",
      "jpg static1 static squarespace insurance\n",
      "Topic 7:\n",
      "repair marketing car auto financial\n",
      "Topic 8:\n",
      "cleaning marketing catering menu financial\n",
      "Topic 9:\n",
      "00 cleaning financial tax pm\n"
     ]
    }
   ],
   "source": [
    "display_components(lsi, tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-negative matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=None, random_state=None, shuffle=False, solver='cd',\n",
       "  tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NMF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "723    Old-Fashioned Brick Oven Pizza in Adrian, MI |...\n",
      "826    Pizza and Wings Delivery Columbia MO Kostaki's...\n",
      "900    Truck, trailer, and auto repair | Sonny’s Serv...\n",
      "974    Auto Repair, Kirksville, MO Kirksville Brake &...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(example_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF also requires tfidf-weighted vectors\n",
    "tfidf_example = tfidf_vectorizer.transform(example_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "pizza brick baked old cooking\n",
      "Topic 1:\n",
      "brake mo repair car wrong\n",
      "Topic 2:\n",
      "trucks fleet towing heavy automotive\n",
      "Topic 3:\n",
      "columbia pizza events 573 sunrise\n"
     ]
    }
   ],
   "source": [
    "# specify number of components\n",
    "# with NMF, n_components must be <= number of documents\n",
    "n_components = 4\n",
    "nmf = NMF(n_components=n_components)\n",
    "nmf_example = nmf.fit_transform(tfidf_example)\n",
    "display_components(nmf, tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old-Fashioned Brick Oven Pizza in Adrian, MI | Piz [8.62522139e-01 1.58644770e-11 2.90992053e-09 0.00000000e+00]\n",
      "Pizza and Wings Delivery Columbia MO Kostaki's Piz [3.94129144e-04 0.00000000e+00 0.00000000e+00 1.01225434e+00]\n",
      "Truck, trailer, and auto repair | Sonny’s Service  [0.00000000e+00 1.68115685e-15 1.05162122e+00 0.00000000e+00]\n",
      "Auto Repair, Kirksville, MO Kirksville Brake & Muf [0.00000000e+00 8.94778284e-01 1.22455262e-16 4.17916518e-17]\n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(example_texts):\n",
    "    print(t[:50], nmf_example[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Create NMF vectors\n",
    "Using the TFIDF vectors, create NMF vectors for the website text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# likely better to use more than 5 components\n",
    "n_components = 10\n",
    "nmf = NMF(n_components=n_components)\n",
    "nmf_vecs = nmf.fit_transform(tfidf_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "menu catering food pizza restaurant\n",
      "Topic 1:\n",
      "insurance financial agency coverage quote\n",
      "Topic 2:\n",
      "painting tree residential commercial landscape\n",
      "Topic 3:\n",
      "law estate legal real attorney\n",
      "Topic 4:\n",
      "design marketing interior portfolio interiors\n",
      "Topic 5:\n",
      "care health life training center\n",
      "Topic 6:\n",
      "jpg static static1 squarespace https\n",
      "Topic 7:\n",
      "repair car auto repairs heating\n",
      "Topic 8:\n",
      "00 pm hair spa salon\n",
      "Topic 9:\n",
      "cleaning window windows boston roofing\n"
     ]
    }
   ],
   "source": [
    "display_components(nmf, tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count shape: (1000, 4891)\n",
      "tfidf shape: (1000, 4891)\n",
      "lsi shape: (1000, 10)\n",
      "nmf shape: (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "# taking inventory of the vectors we have\n",
    "vector_sets = {'count':count_vecs,\n",
    "               'tfidf':tfidf_vecs,\n",
    "               'lsi':lsi_vecs,\n",
    "               'nmf':nmf_vecs}\n",
    "for k, v in vector_sets.items():\n",
    "    print(k, 'shape:',  v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "723    Old-Fashioned Brick Oven Pizza in Adrian, MI |...\n",
      "826    Pizza and Wings Delivery Columbia MO Kostaki's...\n",
      "900    Truck, trailer, and auto repair | Sonny’s Serv...\n",
      "974    Auto Repair, Kirksville, MO Kirksville Brake &...\n",
      "Name: content, dtype: object\n",
      "tfidf shape: (4, 4891)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Old-Fashioned Brick</th>\n",
       "      <th>Pizza and Wings Deli</th>\n",
       "      <th>Truck, trailer, and</th>\n",
       "      <th>Auto Repair, Kirksvi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Old-Fashioned Brick</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.373726</td>\n",
       "      <td>0.007357</td>\n",
       "      <td>0.006656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pizza and Wings Deli</th>\n",
       "      <td>0.373726</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007488</td>\n",
       "      <td>0.053750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Truck, trailer, and</th>\n",
       "      <td>0.007357</td>\n",
       "      <td>0.007488</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.154114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Auto Repair, Kirksvi</th>\n",
       "      <td>0.006656</td>\n",
       "      <td>0.053750</td>\n",
       "      <td>0.154114</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Old-Fashioned Brick   Pizza and Wings Deli  \\\n",
       "Old-Fashioned Brick               1.000000              0.373726   \n",
       "Pizza and Wings Deli              0.373726              1.000000   \n",
       "Truck, trailer, and               0.007357              0.007488   \n",
       "Auto Repair, Kirksvi              0.006656              0.053750   \n",
       "\n",
       "                      Truck, trailer, and   Auto Repair, Kirksvi  \n",
       "Old-Fashioned Brick               0.007357              0.006656  \n",
       "Pizza and Wings Deli              0.007488              0.053750  \n",
       "Truck, trailer, and               1.000000              0.154114  \n",
       "Auto Repair, Kirksvi              0.154114              1.000000  "
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cosine similarity\n",
    "# looking at our examples from above\n",
    "print(example_texts)\n",
    "print('tfidf shape:', tfidf_example.shape)\n",
    "example_sim = cosine_similarity(tfidf_example)\n",
    "# truncate descriptions\n",
    "trunc_example_texts = [x[:20] for x in example_texts.values]\n",
    "pd.DataFrame(example_sim,\n",
    "             index=trunc_example_texts,\n",
    "             columns=trunc_example_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN                        77\n",
       "Professional Services      53\n",
       "Health and Fitness         49\n",
       "Home & Home Improvement    40\n",
       "Construction               40\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have industry category for subset of businesses\n",
    "full_data.type.value_counts(dropna=False).head()\n",
    "#k = 5\n",
    "#top_k_industries = full_data['type'].value_counts().head(n=5)\n",
    "#print(top_k_industries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Which of the four techniques appears to work best?\n",
    "For this more open-ended question, here's a suggestion for a workflow:\n",
    "\n",
    "1) Take inventory of available vectorized data\n",
    "\n",
    "2) Assess sources for \"ground truth\"\n",
    "\n",
    "3) Determine a metric of performance for the techniques\n",
    "\n",
    "4) Analyze and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 21.0\n",
      "tfidf 26.0\n",
      "lsi 51.0\n",
      "nmf 52.0\n"
     ]
    }
   ],
   "source": [
    "vector_sims = {}\n",
    "ind = 'Construction'\n",
    "\n",
    "for m in vector_sets:\n",
    "    vector_sim = cosine_similarity(vector_sets[m])\n",
    "    # remove self-comparison, would automatically up-weight self category\n",
    "    np.fill_diagonal(vector_sim, np.NaN)\n",
    "    vector_sims[m] = vector_sim\n",
    "    s_df = pd.DataFrame(vector_sims[m],\n",
    "            index=full_data.type,\n",
    "            columns=full_data.type)\n",
    "    t = s_df.groupby(level=0, axis=1).mean().groupby(level=0).mean()\n",
    "    print(m, t.loc[ind].rank().loc[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:devpy3]",
   "language": "python",
   "name": "conda-env-devpy3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
