{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In your own words: Computing customer similarity using website text data\n",
    "#### Workshop developed for DSS Austin '19\n",
    "#### By: Ben Batorsky [github](https://github.com/bpben)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional, can run exercise code without these\n",
    "import Stemmer\n",
    "stemmer = Stemmer.Stemmer('english')\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from gensim.models.phrases import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion\n",
    "The data for the workshop comes from a random set of business website text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_pickle(DATA_PATH+'website_text.pkl')\n",
    "w_content = full_data['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_data.content.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({\"We're\": 1, 'Cowboys': 1, 'fans,': 1, 'but': 1, \"we're\": 1, 'not': 1, 'cowboys': 1})\n",
      "Counter({\"we're\": 2, 'cowboys': 2, 'fans,': 1, 'but': 1, 'not': 1})\n"
     ]
    }
   ],
   "source": [
    "# capitalization\n",
    "text = \"We're Cowboys fans, but we're not cowboys\"\n",
    "print(Counter(text.split()))\n",
    "print(Counter(text.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're Cowboys fans, but we're not cowboys\n",
      "Were Cowboys fans but were not cowboys\n"
     ]
    }
   ],
   "source": [
    "# punctuation\n",
    "text = \"We're Cowboys fans, but we're not cowboys\"\n",
    "strip_punct = '[^A-Za-z0-9 ]'\n",
    "print(text)\n",
    "print(re.sub(strip_punct, '', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call 867-5309\n",
      "Call -\n"
     ]
    }
   ],
   "source": [
    "# numbers\n",
    "text = 'Call 867-5309'\n",
    "strip_num = '[0-9]'\n",
    "print(text)\n",
    "print(re.sub(strip_num, '', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out this conference: https://datascience.salon/austin/\n",
      "Check out this conference: \n"
     ]
    }
   ],
   "source": [
    "# urls\n",
    "# regex from textacy: https://github.com/chartbeat-labs/textacy\n",
    "SHORT_URL_REGEX = re.compile(\n",
    "    r\"(?:^|(?<![\\w/.]))\"\n",
    "    # optional scheme\n",
    "    r\"(?:(?:https?://)?)\"\n",
    "    # domain\n",
    "    r\"(?:\\w-?)*?\\w+(?:\\.[a-z]{2,12}){1,3}\"\n",
    "    r\"/+\"\n",
    "    # hash\n",
    "    r\"[^\\s.,?!'\\\"|+]{2,12}\"\n",
    "    r\"(?:$|(?![\\w?!+&/]))\",\n",
    "    flags=re.IGNORECASE)\n",
    "text = 'Check out this conference: https://datascience.salon/austin/'\n",
    "print(text)\n",
    "print(SHORT_URL_REGEX.sub('', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Write your preprocessing script\n",
    "Combine some of the regex expressions (or write your own!) to process the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # url\n",
    "    text = SHORT_URL_REGEX.sub('', text)\n",
    "    # numbers\n",
    "    text = re.sub(strip_num, '', text)\n",
    "    # punctuation\n",
    "    text = re.sub(strip_punct, '', text)\n",
    "    # capitalization\n",
    "    text = text.lower()\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_processed = w_content.apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From text to vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 to 1-grams:  ['have', 'lovely', 'bunch', 'of', 'coconuts']\n",
      "1 to 2-grams:  ['have', 'lovely', 'bunch', 'of', 'coconuts', 'have lovely', 'lovely bunch', 'bunch of', 'of coconuts']\n",
      "1 to 3-grams:  ['have', 'lovely', 'bunch', 'of', 'coconuts', 'have lovely', 'lovely bunch', 'bunch of', 'of coconuts', 'have lovely bunch', 'lovely bunch of', 'bunch of coconuts']\n"
     ]
    }
   ],
   "source": [
    "# n-grams\n",
    "# note: 1-letter words are dropped by default\n",
    "text = ['I have a lovely bunch of coconuts']\n",
    "for n in range(1, 4):\n",
    "    vec = CountVectorizer(ngram_range=(1, n))\n",
    "    print('1 to {}-grams: '.format(n), list(vec.fit(text).vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default (no minimum):  ['have', 'lovely', 'bunch', 'of', 'coconuts', 'pears']\n",
      "Appear in >=20% of docs:  ['have', 'lovely', 'bunch', 'of', 'coconuts']\n",
      "Appear in <=10% of docs:  ['pears']\n"
     ]
    }
   ],
   "source": [
    "# choosing cutoffs\n",
    "texts = ['I have a lovely bunch of coconuts']\n",
    "texts = texts*9\n",
    "texts.append('I have a lovely bunch of pears')\n",
    "vec = CountVectorizer()\n",
    "print('Default (no minimum): ', list(vec.fit(texts).vocabulary_.keys()))\n",
    "vec = CountVectorizer(min_df=.2)\n",
    "print('Appear in >=20% of docs: ', list(vec.fit(texts).vocabulary_.keys()))\n",
    "vec = CountVectorizer(max_df=.1)\n",
    "print('Appear in <=10% of docs: ', list(vec.fit(texts).vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stem of ponies: poni\n",
      "Lemma of ponies: pony\n",
      "\n",
      "Stem of operation: oper\n",
      "Lemma of operation: operation\n",
      "\n",
      "Stem of are: are\n",
      "Lemma of are: be\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# stemming and lemmatizing\n",
    "words = ['ponies', 'operation', 'are']\n",
    "for w in words:\n",
    "    print('Stem of {}: {}'.format(w, stemmer.stemWord(w)))\n",
    "    print('Lemma of {}: {}'.format(w, nlp(w)[0].lemma_))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cowboys ORG\n"
     ]
    }
   ],
   "source": [
    "# entities with spaCy\n",
    "text = \"I'm a Cowboys fan, but I'm not a cowboy\"\n",
    "ents = nlp(text).ents\n",
    "for e in ents:\n",
    "    print(e, e.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with min count = 1:\n",
      "[(b'have a', 1.8055555555555556), (b'lovely bunch', 1.8055555555555556), (b'of pears', 1.0833333333333333)]\n",
      "with min count = 2\n",
      "[(b'have a', 1.4444444444444444), (b'lovely bunch', 1.4444444444444444)]\n"
     ]
    }
   ],
   "source": [
    "# entities with gensim - co-location\n",
    "# This example is a bit odd: Likely not dealing with a bunch of duplicates\n",
    "# Worth noting that, all likelihood being equal, gensim picks the first in a series of bigrams\n",
    "texts = ['have a lovely bunch of coconuts']\n",
    "texts = texts*4\n",
    "texts.append('have a lovely bunch of pears')\n",
    "texts.append('have a lovely bunch of pears')\n",
    "split_texts = [x.split() for x in texts]\n",
    "phrases = Phrases(split_texts, min_count=1, threshold=1)\n",
    "phrases_m2 = Phrases(split_texts, min_count=2, threshold=1)\n",
    "# scores are based on https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.original_scorer\n",
    "# higher = more likely to be a bigram\n",
    "print('with min count = 1:') \n",
    "print([x for x in phrases.export_phrases([split_texts[-1]])])\n",
    "print('with min count = 2')\n",
    "print([x for x in phrases_m2.export_phrases([split_texts[-1]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vectors \n",
      "    are  but  cowboys  cowboysorg  fans  not  we\n",
      "0    2    1        2           0     1    1   2\n",
      "1    2    1        1           1     1    1   2\n",
      "binary vectors \n",
      "    are  but  cowboys  cowboysorg  fans  not  we\n",
      "0    1    1        1           0     1    1   1\n",
      "1    1    1        1           1     1    1   1\n"
     ]
    }
   ],
   "source": [
    "# creating count vectors\n",
    "texts = []\n",
    "text = \"We are Cowboys fans, but we are not cowboys\"\n",
    "text_tagged = \"We are CowboysORG fans, but we are not cowboys\"\n",
    "# phrase without tags, lowercase\n",
    "texts.append(text.lower())\n",
    "# phrase with tags, lowercase\n",
    "texts.append(text_tagged.lower())\n",
    "# utility to display vectorizer\n",
    "def display_vec(vec, data):\n",
    "    df = pd.DataFrame(data.toarray(),\n",
    "                     columns=vec.get_feature_names())\n",
    "    return(df)\n",
    "# count vector\n",
    "vec = CountVectorizer()\n",
    "data = vec.fit_transform(texts)\n",
    "print('count vectors \\n', display_vec(vec, data))\n",
    "# binary count vector\n",
    "b_vec = CountVectorizer(binary=True)\n",
    "data = b_vec.fit_transform(texts)\n",
    "print('binary vectors \\n', display_vec(b_vec, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Remove the stopwords from the above texts\n",
    "Use what we explored above to remove the stopwords from the count vectors of the following texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vectors without stopwords \n",
      "    cowboys  cowboysorg  fans\n",
      "0        2           0     1\n",
      "1        1           1     1\n"
     ]
    }
   ],
   "source": [
    "texts = ['we are cowboys fans, but we are not cowboys',\n",
    " 'we are cowboysorg fans, but we are not cowboys']\n",
    "\n",
    "nostop_vec = CountVectorizer(stop_words='english')\n",
    "data = nostop_vec.fit_transform(texts)\n",
    "print('count vectors without stopwords \\n', display_vec(nostop_vec, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Set limits on the vocabulary to remove potentially irrelevant words\n",
    "With the following set of texts, set a limit to remove unimportant words like \"Patriots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vector with vocab limit \n",
      "    are  cowboys  fans  we\n",
      "0    1        1     1   1\n",
      "1    1        1     0   1\n",
      "2    1        0     1   1\n"
     ]
    }
   ],
   "source": [
    "texts = ['We are Cowboys fans',\n",
    "        'We are cowboys',\n",
    "        'We are Patriots fans']\n",
    "texts_lower = [t.lower() for t in texts]\n",
    "\n",
    "limit_vec = CountVectorizer(min_df=2)\n",
    "data = limit_vec.fit_transform(texts)\n",
    "print('Count vector with vocab limit \\n', display_vec(limit_vec, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF weighting\n",
    "TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['We are Cowboys fans',\n",
    "         'We are Patriots fans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   are  cowboys  fans  patriots  we\n",
      "0    1        1     1         0   1\n",
      "1    1        0     1         1   1\n"
     ]
    }
   ],
   "source": [
    "# calculate term frequency\n",
    "vec = CountVectorizer()\n",
    "count_vectors = vec.fit_transform(texts)\n",
    "count_df = display_vec(vec, count_vectors)\n",
    "print(count_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula for inverse document frequency weight:\n",
    "\n",
    "$$log(\\frac{N}{df(t)}) + 1$$\n",
    "\n",
    "\"smooth\" option ensures no zero-divisions:\n",
    "\n",
    "$$log(\\frac{N+1}{df(t)+1}) + 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are         1.000000\n",
      "cowboys     1.405465\n",
      "fans        1.000000\n",
      "patriots    1.405465\n",
      "we          1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# get inverse document frequency\n",
    "df = np.log(3/(1+count_df.sum()))+1\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        are   cowboys      fans  patriots        we\n",
      "0  0.448321  0.630099  0.448321  0.000000  0.448321\n",
      "1  0.448321  0.000000  0.448321  0.630099  0.448321\n"
     ]
    }
   ],
   "source": [
    "# calculate tfidf\n",
    "tfidf_df = count_df*df\n",
    "# normalize\n",
    "print(tfidf_df.apply(\n",
    "    lambda x: x/np.sqrt(x.dot(x)), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        are   cowboys      fans  patriots        we\n",
      "0  0.448321  0.630099  0.448321  0.000000  0.448321\n",
      "1  0.448321  0.000000  0.448321  0.630099  0.448321\n"
     ]
    }
   ],
   "source": [
    "# now with scikit-learn\n",
    "tfidf = TfidfVectorizer()\n",
    "data = tfidf.fit_transform(texts)\n",
    "print(display_vec(tfidf, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Turn text to vectors\n",
    "Using what we've gone through above, create your own count vectorizer and TFIDF vectorizer.  Apply these vectorizers to the data, and store the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_params = {'min_df': .005, 'max_df': .3, 'stop_words':'english'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(**vector_params)\n",
    "tfidf_vectorizer = TfidfVectorizer(**vector_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vecs = count_vectorizer.fit_transform(w_processed)\n",
    "tfidf_vecs = tfidf_vectorizer.fit_transform(w_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix factorization and topic modelling\n",
    "\n",
    "#### Latent Semantic Indexing\n",
    "In scikit-learn this is implemented as TruncatedSVD, a version of SVD where the top k elements are retained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=2, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TruncatedSVD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812    Basketball Training in Greenville, South Carol...\n",
       "558    Personal Trainer | Nutrition Coach | Bolton, M...\n",
       "610    Siding company in Millersburg, OH | Holmes Sid...\n",
       "946    Glass solutions in Devils Lake, ND  | The Glas...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# couple examples website text\n",
    "# choose some from pretty opposite industries\n",
    "random_state = 9\n",
    "example_inds = ['Health and Fitness', 'Home & Home Improvement']\n",
    "example_idxs = []\n",
    "for ind in example_inds:\n",
    "    ind_data = full_data[full_data.type==ind]\n",
    "    idxs = ind_data.sample(n=2, random_state=random_state).index\n",
    "    example_idxs.extend(idxs.tolist())\n",
    "example_texts = w_content.loc[example_idxs]\n",
    "example_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accepted</th>\n",
       "      <th>action</th>\n",
       "      <th>added</th>\n",
       "      <th>advanced</th>\n",
       "      <th>advantage</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>allowed</th>\n",
       "      <th>answer</th>\n",
       "      <th>application</th>\n",
       "      <th>area</th>\n",
       "      <th>...</th>\n",
       "      <th>websites</th>\n",
       "      <th>weeks</th>\n",
       "      <th>weight</th>\n",
       "      <th>wellness</th>\n",
       "      <th>windows</th>\n",
       "      <th>working</th>\n",
       "      <th>workmanship</th>\n",
       "      <th>worth</th>\n",
       "      <th>year</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034436</td>\n",
       "      <td>0.036078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04141</td>\n",
       "      <td>0.037496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.02398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036615</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.034227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.06906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082503</td>\n",
       "      <td>0.08393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129506</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131218</td>\n",
       "      <td>0.338251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.041274</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.036495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.067037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107877</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.079786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.02515</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 392 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accepted   action     added  advanced  advantage  afternoon  allowed  \\\n",
       "0  0.000000  0.00000  0.000000  0.034436   0.036078   0.000000  0.00000   \n",
       "1  0.000000  0.06906  0.000000  0.000000   0.000000   0.082503  0.08393   \n",
       "2  0.041274  0.00000  0.036495  0.000000   0.000000   0.000000  0.00000   \n",
       "3  0.000000  0.00000  0.000000  0.000000   0.000000   0.000000  0.00000   \n",
       "\n",
       "     answer  application      area    ...     websites     weeks    weight  \\\n",
       "0  0.000000     0.000000  0.042093    ...      0.04141  0.037496  0.000000   \n",
       "1  0.000000     0.129506  0.000000    ...      0.00000  0.000000  0.131218   \n",
       "2  0.067037     0.000000  0.000000    ...      0.00000  0.000000  0.000000   \n",
       "3  0.000000     0.000000  0.071538    ...      0.00000  0.000000  0.000000   \n",
       "\n",
       "   wellness   windows  working  workmanship     worth     year     young  \n",
       "0  0.000000  0.000000  0.02398     0.000000  0.036615  0.00000  0.034227  \n",
       "1  0.338251  0.000000  0.00000     0.000000  0.000000  0.00000  0.000000  \n",
       "2  0.000000  0.107877  0.00000     0.079786  0.000000  0.02515  0.000000  \n",
       "3  0.000000  0.000000  0.00000     0.000000  0.000000  0.00000  0.000000  \n",
       "\n",
       "[4 rows x 392 columns]"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSI requires tfidf-weighted vectors, use from above\n",
    "tfidf_example = tfidf_vectorizer.transform(example_texts)\n",
    "# create display of examples\n",
    "display_example = display_vec(tfidf_vectorizer, tfidf_example)\n",
    "# for clarity, drop vocab that does not occur\n",
    "display_example.loc[:, (display_example.sum(axis=0)>0).values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_components(model, word_features, top_display=5):\n",
    "    # utility for displaying respresentative words per component\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        top_words_idx = topic.argsort()[::-1][:top_display]\n",
    "        top_words = [word_features[i] for i in top_words_idx]\n",
    "        print(\" \".join(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "training basketball coaching glass skills\n",
      "Topic 1:\n",
      "siding glass contractors lake products\n",
      "Topic 2:\n",
      "siding contractors products installation gutters\n",
      "Topic 3:\n",
      "basketball skills training camps combine\n"
     ]
    }
   ],
   "source": [
    "# specify number of components\n",
    "n_components = 4\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=random_state)\n",
    "svd_example = svd.fit_transform(tfidf_example)\n",
    "display_components(svd, tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Basketball Training in Greenville, South Carolina</th>\n",
       "      <td>0.712078</td>\n",
       "      <td>-0.203583</td>\n",
       "      <td>-0.073317</td>\n",
       "      <td>0.667925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personal Trainer | Nutrition Coach | Bolton, MA A</th>\n",
       "      <td>0.634499</td>\n",
       "      <td>-0.444886</td>\n",
       "      <td>0.102180</td>\n",
       "      <td>-0.623736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Siding company in Millersburg, OH | Holmes Siding</th>\n",
       "      <td>0.320416</td>\n",
       "      <td>0.634437</td>\n",
       "      <td>0.702907</td>\n",
       "      <td>-0.027300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glass solutions in Devils Lake, ND  | The Glass Sh</th>\n",
       "      <td>0.358093</td>\n",
       "      <td>0.625431</td>\n",
       "      <td>-0.664210</td>\n",
       "      <td>-0.198572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           0         1  \\\n",
       "Basketball Training in Greenville, South Carolina   0.712078 -0.203583   \n",
       "Personal Trainer | Nutrition Coach | Bolton, MA A   0.634499 -0.444886   \n",
       "Siding company in Millersburg, OH | Holmes Siding   0.320416  0.634437   \n",
       "Glass solutions in Devils Lake, ND  | The Glass Sh  0.358093  0.625431   \n",
       "\n",
       "                                                           2         3  \n",
       "Basketball Training in Greenville, South Carolina  -0.073317  0.667925  \n",
       "Personal Trainer | Nutrition Coach | Bolton, MA A   0.102180 -0.623736  \n",
       "Siding company in Millersburg, OH | Holmes Siding   0.702907 -0.027300  \n",
       "Glass solutions in Devils Lake, ND  | The Glass Sh -0.664210 -0.198572  "
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for display\n",
    "pd.DataFrame(svd_example,\n",
    "             index=[t[:50] for t in example_texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Create LSI vectors\n",
    "Using the TFIDF vectors from above, create LSI vectors for the website text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# likely better to use more than 4 components\n",
    "n_components = 10\n",
    "lsi = TruncatedSVD(n_components=n_components)\n",
    "lsi_vecs = lsi.fit_transform(tfidf_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x4591 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 100650 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "care products read pm health\n",
      "Topic 1:\n",
      "cleaning repair carpet commercial residential\n",
      "Topic 2:\n",
      "insurance auto coverage cleaning repair\n",
      "Topic 3:\n",
      "insurance menu food pizza catering\n",
      "Topic 4:\n",
      "cleaning care insurance health carpet\n",
      "Topic 5:\n",
      "repair auto care massage car\n",
      "Topic 6:\n",
      "repair auto cleaning car computer\n",
      "Topic 7:\n",
      "care dental law patient dr\n",
      "Topic 8:\n",
      "spa law hair estate salon\n",
      "Topic 9:\n",
      "massage pest therapy lawn control\n"
     ]
    }
   ],
   "source": [
    "display_components(lsi, tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-negative matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=None, random_state=None, shuffle=False, solver='cd',\n",
       "  tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NMF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "812    Basketball Training in Greenville, South Carol...\n",
      "558    Personal Trainer | Nutrition Coach | Bolton, M...\n",
      "610    Siding company in Millersburg, OH | Holmes Sid...\n",
      "946    Glass solutions in Devils Lake, ND  | The Glas...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(example_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF also requires tfidf-weighted vectors\n",
    "tfidf_example = tfidf_vectorizer.transform(example_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "training basketball skills camps combine\n",
      "Topic 1:\n",
      "glass lake nd shop mail\n",
      "Topic 2:\n",
      "siding contractors products installation gutters\n",
      "Topic 3:\n",
      "coaching wellness choices personal foods\n"
     ]
    }
   ],
   "source": [
    "# specify number of components\n",
    "# with NMF, n_components must be <= number of documents\n",
    "n_components = 4\n",
    "nmf = NMF(n_components=n_components)\n",
    "nmf_example = nmf.fit_transform(tfidf_example)\n",
    "display_components(nmf, tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Basketball Training in Greenville, South Carolina</th>\n",
       "      <td>0.772661</td>\n",
       "      <td>1.588835e-09</td>\n",
       "      <td>1.367966e-09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personal Trainer | Nutrition Coach | Bolton, MA A</th>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.128060e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Siding company in Millersburg, OH | Holmes Siding</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.714173e-09</td>\n",
       "      <td>1.073865e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glass solutions in Devils Lake, ND  | The Glass Sh</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.722972e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.395137e-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           0             1  \\\n",
       "Basketball Training in Greenville, South Carolina   0.772661  1.588835e-09   \n",
       "Personal Trainer | Nutrition Coach | Bolton, MA A   0.000043  0.000000e+00   \n",
       "Siding company in Millersburg, OH | Holmes Siding   0.000000  1.714173e-09   \n",
       "Glass solutions in Devils Lake, ND  | The Glass Sh  0.000000  9.722972e-01   \n",
       "\n",
       "                                                               2             3  \n",
       "Basketball Training in Greenville, South Carolina   1.367966e-09  0.000000e+00  \n",
       "Personal Trainer | Nutrition Coach | Bolton, MA A   0.000000e+00  1.128060e+00  \n",
       "Siding company in Millersburg, OH | Holmes Siding   1.073865e+00  0.000000e+00  \n",
       "Glass solutions in Devils Lake, ND  | The Glass Sh  0.000000e+00  2.395137e-16  "
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(nmf_example,\n",
    "             index=[t[:50] for t in example_texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Create NMF vectors\n",
    "Using the TFIDF vectors, create NMF vectors for the website text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# likely better to use more than 5 components\n",
    "n_components = 10\n",
    "nmf = NMF(n_components=n_components)\n",
    "nmf_vecs = nmf.fit_transform(tfidf_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "marketing media solutions digital consulting\n",
      "Topic 1:\n",
      "cleaning carpet commercial clean janitorial\n",
      "Topic 2:\n",
      "insurance coverage agency auto quote\n",
      "Topic 3:\n",
      "menu food pm pizza catering\n",
      "Topic 4:\n",
      "massage spa hair salon skin\n",
      "Topic 5:\n",
      "repair auto car repairs equipment\n",
      "Topic 6:\n",
      "design products custom lawn construction\n",
      "Topic 7:\n",
      "care health dental dr patient\n",
      "Topic 8:\n",
      "law estate real legal attorney\n",
      "Topic 9:\n",
      "training fitness preschool life coaching\n"
     ]
    }
   ],
   "source": [
    "display_components(nmf, tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count shape: (1000, 4591)\n",
      "tfidf shape: (1000, 4591)\n",
      "lsi shape: (1000, 10)\n",
      "nmf shape: (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "# taking inventory of the vectors we have\n",
    "vector_sets = {'count':count_vecs,\n",
    "               'tfidf':tfidf_vecs,\n",
    "               'lsi':lsi_vecs,\n",
    "               'nmf':nmf_vecs}\n",
    "for k, v in vector_sets.items():\n",
    "    print(k, 'shape:',  v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "812    Basketball Training in Greenville, South Carol...\n",
      "558    Personal Trainer | Nutrition Coach | Bolton, M...\n",
      "610    Siding company in Millersburg, OH | Holmes Sid...\n",
      "946    Glass solutions in Devils Lake, ND  | The Glas...\n",
      "Name: content, dtype: object\n",
      "tfidf shape: (4, 4591)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Basketball Training</th>\n",
       "      <th>Personal Trainer | N</th>\n",
       "      <th>Siding company in Mi</th>\n",
       "      <th>Glass solutions in D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Basketball Training</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.118283</td>\n",
       "      <td>0.029231</td>\n",
       "      <td>0.043730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personal Trainer | N</th>\n",
       "      <td>0.118283</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009903</td>\n",
       "      <td>0.004952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Siding company in Mi</th>\n",
       "      <td>0.029231</td>\n",
       "      <td>0.009903</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glass solutions in D</th>\n",
       "      <td>0.043730</td>\n",
       "      <td>0.004952</td>\n",
       "      <td>0.050079</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Basketball Training   Personal Trainer | N  \\\n",
       "Basketball Training               1.000000              0.118283   \n",
       "Personal Trainer | N              0.118283              1.000000   \n",
       "Siding company in Mi              0.029231              0.009903   \n",
       "Glass solutions in D              0.043730              0.004952   \n",
       "\n",
       "                      Siding company in Mi  Glass solutions in D  \n",
       "Basketball Training               0.029231              0.043730  \n",
       "Personal Trainer | N              0.009903              0.004952  \n",
       "Siding company in Mi              1.000000              0.050079  \n",
       "Glass solutions in D              0.050079              1.000000  "
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cosine similarity\n",
    "# looking at our examples from above\n",
    "print(example_texts)\n",
    "print('tfidf shape:', tfidf_example.shape)\n",
    "example_sim = cosine_similarity(tfidf_example)\n",
    "# truncate descriptions\n",
    "trunc_example_texts = [x[:20] for x in example_texts.values]\n",
    "pd.DataFrame(example_sim,\n",
    "             index=trunc_example_texts,\n",
    "             columns=trunc_example_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN                        77\n",
       "Professional Services      53\n",
       "Health and Fitness         49\n",
       "Construction               40\n",
       "Home & Home Improvement    40\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have industry category for subset of businesses\n",
    "full_data.type.value_counts(dropna=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Which of the four techniques appears to work best?\n",
    "For this more open-ended question, here's a suggestion for a workflow:\n",
    "\n",
    "1) Take inventory of available vectorized data\n",
    "\n",
    "2) Assess sources for \"ground truth\"\n",
    "\n",
    "3) Determine a metric of performance for the techniques\n",
    "\n",
    "4) Analyze and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 84.0\n",
      "tfidf 85.0\n",
      "lsi 82.0\n",
      "nmf 79.0\n"
     ]
    }
   ],
   "source": [
    "vector_sims = {}\n",
    "ind = 'Home & Home Improvement'\n",
    "\n",
    "for m in vector_sets:\n",
    "    # compute cosine similarity\n",
    "    vector_sim = cosine_similarity(vector_sets[m])\n",
    "    # remove self-comparison, would automatically up-weight self category\n",
    "    np.fill_diagonal(vector_sim, np.NaN)\n",
    "    vector_sims[m] = vector_sim\n",
    "    s_df = pd.DataFrame(vector_sims[m],\n",
    "            index=full_data.type,\n",
    "            columns=full_data.type)\n",
    "    t = s_df.groupby(level=0, axis=1).mean().groupby(level=0).mean()\n",
    "    print(m, t.loc[ind].rank().loc[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at a case here\n",
    "same_ind = np.where(s_df.index==ind)[0]\n",
    "zero_similarity = np.where(s_df.loc[ind, ind]==0)\n",
    "same_x = same_ind[zero_similarity[0][0]]\n",
    "same_y = same_ind[zero_similarity[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25     High-quality masonry work in Hilton, NY  | Ups...\n",
       "237    Ames Clean Company Home About Us Services Clea...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.iloc[\n",
    "    [same_x, same_y]].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:devpy3]",
   "language": "python",
   "name": "conda-env-devpy3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
